<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>S. Partarrieu | Hyperspherical Variational Auto-Encoders</title>
<meta name="description" content="">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/blog/2023/hyperspherical-VAEs/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>



  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-J228W495ML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-J228W495ML');
  </script>




    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://sebastianpartarrieu.github.io//">
       S. Partarrieu
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/Publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <!-- _layouts/post.html -->






<div class="post">

  <header class="post-header">
    <h1 class="post-title">Hyperspherical Variational Auto-Encoders</h1>
    <p class="post-meta">April 6, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
      
        ·  
        
        <a href="/blog/tag/math">          <i class="fas fa-hashtag fa-sm"></i> math</a>  
          
        <a href="/blog/tag/code">          <i class="fas fa-hashtag fa-sm"></i> code</a>  
          
        <a href="/blog/tag/papers">          <i class="fas fa-hashtag fa-sm"></i> papers</a>  
          
      

      
    </p>
  </header>

  <article class="post-content">
    <div class="row justify-content-sm-center">
        <img class="img-fluid rounded z-depth-1" zoomable="true" src="/assets/img/hyperspherical_VAE_stable_diffusion.jpeg" alt="" title="H-VAE" width="400" height="200">
    </div>
<div class="caption">
    Hyperspherical variational autoencoders (generated using Stable Diffusion)
</div>

<p>“<strong>OMG</strong>, have you seen [insert latest OpenAI GPT model], isn’t it amazing !???”</p>

<p>For all those who are getting a little irritated at the sheer quantity of noise in the recent hype cycle, and of all the former web3.0/crypto-heads now going “all-in” on AI: you’ve come to the right place. Let’s forget about the hype and about <a href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/" target="_blank" rel="noopener noreferrer">how we’re all supposedly going to die</a>, heck let’s even forget about the transformer architecture and GPT-style generative models. Instead, why don’t we lay back, do some math, and have a look at another type of probabilistic generative model - the variational autoencoder (VAE). Don’t worry, this won’t be the million-th blog post explaining how these work, instead I’ll be diving into a recent(ish) flavor of VAE: the <a href="https://arxiv.org/pdf/1804.00891.pdf" target="_blank" rel="noopener noreferrer">Hyperspherical VAE</a>. These modify the prior and posterior to use non-Gaussians distributions such as the von Mises-Fisher (vMF) distributions, which can be interesting when we expect the latent structure to exhibit some hyperspherical properties. Plus, not only does hyperspherical latent structure <em>sound</em> cool but it also gives some neat visualizations. So, if you have 5 mins of your precious time to spare, we can go through the paper together and I’ll show you a few fun experiments we were able to implement which allowed us to play with the original code and try to bump performance a little.</p>

<h1 id="a-very-brief-primer-on-vaes">A <em>very</em> brief primer on VAEs</h1>
<p>If you’re still reading at this point, I’m guessing you weren’t bored by the ML jargon above and hopefully this means you also already kind of know how a VAE works. For our purposes, a VAE looks something like this:</p>

<div class="row justify-content-sm-center">
        <img class="img-fluid rounded z-depth-1" zoomable="true" src="/assets/img/VAE_blog.png" alt="" title="VAE" width="500" height="300">
    </div>
<div class="caption">
    Simplified VAE
</div>

<p>You take input data \(X \in \mathbb{R}^{n \times d}\) (\(n\) samples and \(d\) dimensions) which you represent by a latent variable \(Z\). Now, in a traditional autoencoder, \(Z \in \mathbb{R}^{n \times d_{latent}}\) will usually be the output of a neural network that simply has an output dimension \(d_{latent}\) which is smaller than \(d\). You compress the data to a latent space with fewer dimensions, then de-compress it using a neural network decoder back into the original space. The <em>variational</em> framework changes this by considering <strong>probabilistic representations</strong> for \(X\) and \(Z\). This means each input will be encoded into a distribution rather than a single value. You may be wondering at this point, why is using probabilities interesting and why should we care about having a latent distributional encoding?</p>

<p>Well, if we consider \(X\) to follow a certain distribution, we can frame the problem as maximizing the log-likelihood of the data \(\log p_{\tau}(x)\) where \(\tau\) parametrizes our input data distribution. This is interesting as if we have the “optimal” parameter that adequately describes our input distribution, then we can sample from the input space directly which means generating new images (hence the name probabilistic generative model). Since the images will be drawn from the “same” distribution as our training set, hopefully this means these won’t look like random noise. The problem is that we don’t know how to optimize \(p_{\tau}(x)\) directly. Let’s imagine an input space of images of cats, you can’t really say “let’s find the best mean and stdev for a multi dimensional gaussian that fits these images” - no gaussian distribution would ever fit that remotely well. Instead, we can consider a latent variable \(Z\) that generates our observations. The nice thing is, we can imagine this latent variable to follow any distribution we would like. This means if we can reframe the optimization of our likelihood to include terms with respect to our latent variable, we have a fighting chance of being able to include reasonable prior assumptions and actually compute the thing.</p>

<p>More mathematically, consider \(\log p_{\tau}(x)\) within the variational framework: we marginalize over the latent space distribution \(p_{\tau}(x) = \int_{z}^{} p_{\tau}(x, z) \,dz\) and rewrite this is as \(p_{\tau}(x) = \int_{z}^{} p_{\tau}(x \| z)p_{\tau}(z) \,dz\). Without going into further detail (see <a href="https://en.wikipedia.org/wiki/Variational_autoencoder#:~:text=Variational%20autoencoders%20are%20probabilistic%20generative,first%20and%20second%20component%20respectively." target="_blank" rel="noopener noreferrer">Wiki</a>), you can optimize a lower bound of the log of this integral by maximizing the Evidence Lower Bound (ELBO) \(\mathbb{E}_{q_{\psi}(z \| x)}[ \log p_{\theta}(x \| z)] - KL(q_{\psi}(z \| x) \| p_{\tau}(z))\). Here, \(q_{\psi}(z \| x)\) represents our approximate posterior distribution (which is why we use q and not p). This approximate posterior is taken from a family of distributions (e.g. gaussian or vMF as we will see later) and we parametrize it by \(\psi\). In practice this parametrization refers to the encoder neural network that will output the parameters of the posterior distribution given an input.</p>

<p>-&gt; link to the fact that we use the vMF as the prior on our latent distribution instead of a gaussian</p>
<h2 id="vmf-distribution">vMF distribution</h2>
<p>The von-Mises Ficher distribution is interesting to consider as a prior for a number of reasons. First, it is not a gaussian. This may sound kind of dumb but honestly, it is not a bad first-order approximation to say that sometimes the best you can do to try and improve a certain method is to where the algorithm relies on the assumption that the underlying distribution is gaussian and <strong>ditch</strong> that. A little more seriously, the von-Mises Fisher is adapted for hyperspherical distributions.</p>

<h2 id="kl-divergence-and-vmf-sampling">KL divergence and vMF sampling</h2>

<h2 id="modifying-the-sampling-mechanism-from-the-original-paper">Modifying the sampling mechanism from the original paper</h2>

<h1 id="some-experiments">Some experiments</h1>

<h2 id="mnist-reconstruction">MNIST reconstruction</h2>
<p>(Yes, we’re <em>still</em> using MNIST for experiments, even in 2023…)</p>

<h2 id="having-a-look-at-the-latent-space">Having a look at the latent space</h2>

<p><em>Originally, this was a short project carried out by <a href="https://www.linkedin.com/in/emma-bou-hanna/" target="_blank" rel="noopener noreferrer">Emma Bou Hanna</a> &amp; me (Sebastian Partarrieu) for the <a href="https://www.master-mva.com/cours/computational-statistics/" target="_blank" rel="noopener noreferrer">Computational Statistics course</a> of the MVA master’s degree.</em></p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    © Copyright 2023 Sebastian A. Partarrieu.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
    Last updated: July 06, 2023.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
